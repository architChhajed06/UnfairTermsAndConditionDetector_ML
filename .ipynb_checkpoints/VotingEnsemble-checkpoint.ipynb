{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "21db5ea2-a1d6-4669-bb19-2b805c6c1ff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Train XGBoost, LightGBM, SVM (with class weights + calibration), and a Voting ensemble\n",
    "on the LawInformedAI/claudette_tos dataset from HuggingFace.\n",
    "\n",
    "Save models and vectorizer at the end.\n",
    "\"\"\"\n",
    "import os\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import classification_report, precision_recall_curve, auc, confusion_matrix\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "# ---------- Settings ----------\n",
    "RANDOM_STATE = 42\n",
    "TEST_SIZE = 0.20\n",
    "TFIDF_NGRAMS = (1, 2)           # unigrams + bigrams\n",
    "MAX_FEATURES = 50000            # adjust if memory-constrained\n",
    "USE_SMOTE = False               # if True, will perform oversampling on train set\n",
    "SAVE_DIR = \"./models\"\n",
    "os.makedirs(SAVE_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7a201ae8-3be0-457c-97bf-fc9b577ad275",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 100%|████| 9414/9414 [00:00<00:00, 716531.37 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns in dataset: ['text', 'label']\n",
      "Example counts:\n",
      " label\n",
      "0    8382\n",
      "1    1032\n",
      "Name: count, dtype: int64\n",
      "Shape: (9414, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# ---------- Load dataset from HuggingFace ----------\n",
    "# NOTE: This uses the dataset id you provided; it loads the 'train' split (≈9.41k rows).\n",
    "ds = load_dataset(\"LawInformedAI/claudette_tos\", split=\"train\")\n",
    "# Convert to pandas\n",
    "df = ds.to_pandas()\n",
    "\n",
    "# Columns: typically 'text' and 'label' (int 0/1)\n",
    "print(\"Columns in dataset:\", df.columns.tolist())\n",
    "print(\"Example counts:\\n\", df['label'].value_counts(normalize=False))\n",
    "print(\"Shape:\", df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eb1a6065-4592-47ef-a730-d2d9e9ce6965",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- Prepare data ----------\n",
    "# Make sure the text column is named correctly; adjust if your column name differs\n",
    "TEXT_COL = \"text\"\n",
    "LABEL_COL = \"label\"\n",
    "\n",
    "X = df[TEXT_COL].astype(str).values\n",
    "y = df[LABEL_COL].astype(int).values\n",
    "\n",
    "# stratified split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=TEST_SIZE, random_state=RANDOM_STATE, stratify=y\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0aee68c4-c3b1-4e37-81c4-3fe909d424b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF shape: (7531, 25900)\n"
     ]
    }
   ],
   "source": [
    "# ---------- TF-IDF vectorization ----------\n",
    "vectorizer = TfidfVectorizer(ngram_range=TFIDF_NGRAMS, max_features=MAX_FEATURES, min_df=2)\n",
    "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
    "X_test_tfidf = vectorizer.transform(X_test)\n",
    "\n",
    "print(\"TF-IDF shape:\", X_train_tfidf.shape)\n",
    "\n",
    "# Optional: SMOTE (uncomment if you want to use oversampling)\n",
    "if USE_SMOTE:\n",
    "    from imblearn.over_sampling import SMOTE\n",
    "    sm = SMOTE(random_state=RANDOM_STATE)\n",
    "    X_train_tfidf, y_train = sm.fit_resample(X_train_tfidf, y_train)\n",
    "    print(\"After SMOTE, train shape:\", X_train_tfidf.shape, np.bincount(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "873d43e2-5224-419d-b92d-8099f06eced7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- Utility: evaluation printer ----------\n",
    "def evaluate_and_print(name, model, X_val, y_val):\n",
    "    \"\"\"\n",
    "    Print classification report + PR AUC and confusion matrix.\n",
    "    If model has predict_proba use that for PR-AUC; otherwise use decision_function.\n",
    "    \"\"\"\n",
    "    y_pred = model.predict(X_val)\n",
    "    print(f\"\\n=== {name} ===\")\n",
    "    print(classification_report(y_val, y_pred, digits=4))\n",
    "    # Confusion matrix\n",
    "    cm = confusion_matrix(y_val, y_pred)\n",
    "    print(\"Confusion matrix:\\n\", cm)\n",
    "    # PR AUC\n",
    "    try:\n",
    "        if hasattr(model, \"predict_proba\"):\n",
    "            y_scores = model.predict_proba(X_val)[:, 1]\n",
    "        else:\n",
    "            # fallback to decision_function (SVM)\n",
    "            y_scores = model.decision_function(X_val)\n",
    "        precision, recall, _ = precision_recall_curve(y_val, y_scores)\n",
    "        pr_auc = auc(recall, precision)\n",
    "        print(f\"Precision-Recall AUC: {pr_auc:.4f}\")\n",
    "    except Exception as e:\n",
    "        print(\"Could not compute PR-AUC:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6e52250f-e66c-4764-ab14-1d7d9e528892",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training XGBoost...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/xgboost/training.py:183: UserWarning: [22:29:07] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== XGBoost ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9681    0.9577    0.9628      1677\n",
      "           1     0.6830    0.7427    0.7116       206\n",
      "\n",
      "    accuracy                         0.9341      1883\n",
      "   macro avg     0.8255    0.8502    0.8372      1883\n",
      "weighted avg     0.9369    0.9341    0.9353      1883\n",
      "\n",
      "Confusion matrix:\n",
      " [[1606   71]\n",
      " [  53  153]]\n",
      "Precision-Recall AUC: 0.7615\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['./models/xgb_claudette.joblib']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ---------- Model 1: XGBoost ----------\n",
    "# handle imbalance via scale_pos_weight = (neg_count / pos_count)\n",
    "neg = (y_train == 0).sum()\n",
    "pos = (y_train == 1).sum()\n",
    "scale_pos_weight = neg / max(1, pos)\n",
    "\n",
    "xgb = XGBClassifier(\n",
    "    n_estimators=300,\n",
    "    max_depth=6,\n",
    "    learning_rate=0.05,\n",
    "    objective=\"binary:logistic\",\n",
    "    eval_metric=\"logloss\",\n",
    "    use_label_encoder=False,\n",
    "    scale_pos_weight=scale_pos_weight,\n",
    "    random_state=RANDOM_STATE,\n",
    "    n_jobs=-1,\n",
    ")\n",
    "print(\"Training XGBoost...\")\n",
    "xgb.fit(X_train_tfidf, y_train)\n",
    "evaluate_and_print(\"XGBoost\", xgb, X_test_tfidf, y_test)\n",
    "joblib.dump(xgb, os.path.join(SAVE_DIR, \"xgb_claudette.joblib\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "66ecdbe8-38c4-41fd-83b8-cba60eebf49d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training LightGBM...\n",
      "[LightGBM] [Info] Number of positive: 826, number of negative: 6705\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.076001 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 66807\n",
      "[LightGBM] [Info] Number of data points in the train set: 7531, number of used features: 2420\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[50]\tvalid_0's binary_logloss: 0.299598\n",
      "[100]\tvalid_0's binary_logloss: 0.217378\n",
      "[150]\tvalid_0's binary_logloss: 0.186309\n",
      "[200]\tvalid_0's binary_logloss: 0.171015\n",
      "[250]\tvalid_0's binary_logloss: 0.162707\n",
      "[300]\tvalid_0's binary_logloss: 0.159531\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[291]\tvalid_0's binary_logloss: 0.159334\n",
      "\n",
      "=== LightGBM ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9673    0.9708    0.9690      1677\n",
      "           1     0.7550    0.7330    0.7438       206\n",
      "\n",
      "    accuracy                         0.9448      1883\n",
      "   macro avg     0.8612    0.8519    0.8564      1883\n",
      "weighted avg     0.9441    0.9448    0.9444      1883\n",
      "\n",
      "Confusion matrix:\n",
      " [[1628   49]\n",
      " [  55  151]]\n",
      "Precision-Recall AUC: 0.8032\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['./models/lgbm_claudette.joblib']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ---------- Model 2: LightGBM ----------\n",
    "from lightgbm import early_stopping, log_evaluation\n",
    "lgbm = LGBMClassifier(\n",
    "    n_estimators=300,\n",
    "    max_depth=-1,\n",
    "    learning_rate=0.05,\n",
    "    objective=\"binary\",\n",
    "    class_weight=\"balanced\",  # alternatively use scale_pos_weight\n",
    "    random_state=RANDOM_STATE,\n",
    "    n_jobs=-1,\n",
    ")\n",
    "print(\"Training LightGBM...\")\n",
    "lgbm.fit(\n",
    "    X_train_tfidf, \n",
    "    y_train,\n",
    "    eval_set=[(X_test_tfidf, y_test)],\n",
    "    callbacks=[early_stopping(stopping_rounds=30), log_evaluation(50)]\n",
    ")\n",
    "evaluate_and_print(\"LightGBM\", lgbm, X_test_tfidf, y_test)\n",
    "joblib.dump(lgbm, os.path.join(SAVE_DIR, \"lgbm_claudette.joblib\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d09a3d2b-198d-4e51-8a9d-ef925391005e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training SVM (LinearSVC) with calibration (this can take some time)...\n",
      "\n",
      "=== SVM (Calibrated LinearSVC) ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9639    0.9863    0.9749      1677\n",
      "           1     0.8623    0.6990    0.7721       206\n",
      "\n",
      "    accuracy                         0.9549      1883\n",
      "   macro avg     0.9131    0.8427    0.8735      1883\n",
      "weighted avg     0.9528    0.9549    0.9528      1883\n",
      "\n",
      "Confusion matrix:\n",
      " [[1654   23]\n",
      " [  62  144]]\n",
      "Precision-Recall AUC: 0.8297\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['./models/svm_calibrated_claudette.joblib']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# ---------- Model 3: SVM (LinearSVC) with calibration ----------\n",
    "# LinearSVC is fast and supports sparse input; class_weight='balanced' to handle class imbalance\n",
    "svc = LinearSVC(class_weight=\"balanced\", max_iter=20000, random_state=RANDOM_STATE)\n",
    "# Calibrate to get probabilities (useful for PR curves / voting soft)\n",
    "svc_cal = CalibratedClassifierCV(estimator=svc, cv=3, method=\"sigmoid\")\n",
    "print(\"Training SVM (LinearSVC) with calibration (this can take some time)...\")\n",
    "svc_cal.fit(X_train_tfidf, y_train)\n",
    "evaluate_and_print(\"SVM (Calibrated LinearSVC)\", svc_cal, X_test_tfidf, y_test)\n",
    "joblib.dump(svc_cal, os.path.join(SAVE_DIR, \"svm_calibrated_claudette.joblib\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3c04105-6f5c-4e6b-94d7-60de16affd82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- Ensemble: Soft Voting (XGB + LGBM + SVM-calibrated) ----------\n",
    "# Create fresh estimator instances (voting will fit them), but it's okay to reuse trained ones if you prefer to skip refit\n",
    "voting = VotingClassifier(\n",
    "    estimators=[\n",
    "        (\"xgb\", XGBClassifier(\n",
    "            n_estimators=200, max_depth=6, learning_rate=0.05,\n",
    "            use_label_encoder=False, eval_metric=\"logloss\", scale_pos_weight=scale_pos_weight, random_state=RANDOM_STATE)),\n",
    "        (\"lgbm\", LGBMClassifier(n_estimators=200, learning_rate=0.05, class_weight=\"balanced\", random_state=RANDOM_STATE)),\n",
    "        (\"svm\", CalibratedClassifierCV(estimator=LinearSVC(class_weight=\"balanced\", max_iter=20000, random_state=RANDOM_STATE), cv=3)),\n",
    "    ],\n",
    "    voting=\"soft\",\n",
    "    n_jobs=-1,\n",
    ")\n",
    "print(\"Training Voting ensemble (soft voting)...\")\n",
    "voting.fit(X_train_tfidf, y_train)\n",
    "evaluate_and_print(\"Voting Ensemble (XGB+LGBM+SVM)\", voting, X_test_tfidf, y_test)\n",
    "joblib.dump(voting, os.path.join(SAVE_DIR, \"voting_ensemble_claudette.joblib\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f117ddbc-45c0-4ffc-bee0-60ad15456efd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
