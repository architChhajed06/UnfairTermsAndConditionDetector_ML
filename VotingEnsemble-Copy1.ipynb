{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "21db5ea2-a1d6-4669-bb19-2b805c6c1ff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Train XGBoost, LightGBM, SVM (with class weights + calibration), and a Voting ensemble\n",
    "on the LawInformedAI/claudette_tos dataset from HuggingFace.\n",
    "\n",
    "Save models and vectorizer at the end.\n",
    "\"\"\"\n",
    "import os\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score,\n",
    "    classification_report, confusion_matrix, precision_recall_curve, auc\n",
    ")\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "# ---------- Settings ----------\n",
    "RANDOM_STATE = 42\n",
    "TEST_SIZE = 0.20\n",
    "TFIDF_NGRAMS = (1, 2)           # unigrams + bigrams\n",
    "MAX_FEATURES = 50000            # adjust if memory-constrained\n",
    "USE_SMOTE = False               # if True, will perform oversampling on train set\n",
    "SAVE_DIR = \"./models\"\n",
    "os.makedirs(SAVE_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "52bbcc58-395c-4433-928a-d7ec74614391",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating a results array to store the results\n",
    "results = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7a201ae8-3be0-457c-97bf-fc9b577ad275",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns in dataset: ['text', 'label']\n",
      "Example counts:\n",
      " label\n",
      "0    8382\n",
      "1    1032\n",
      "Name: count, dtype: int64\n",
      "Shape: (9414, 2)\n"
     ]
    }
   ],
   "source": [
    "# ---------- Load dataset from HuggingFace ----------\n",
    "# NOTE: This uses the dataset id you provided; it loads the 'train' split (â‰ˆ9.41k rows).\n",
    "ds = load_dataset(\"LawInformedAI/claudette_tos\", split=\"train\")\n",
    "# Convert to pandas\n",
    "df = ds.to_pandas()\n",
    "\n",
    "# Columns: typically 'text' and 'label' (int 0/1)\n",
    "print(\"Columns in dataset:\", df.columns.tolist())\n",
    "print(\"Example counts:\\n\", df['label'].value_counts(normalize=False))\n",
    "print(\"Shape:\", df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "eb1a6065-4592-47ef-a730-d2d9e9ce6965",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- Prepare data ----------\n",
    "# Make sure the text column is named correctly; adjust if your column name differs\n",
    "TEXT_COL = \"text\"\n",
    "LABEL_COL = \"label\"\n",
    "\n",
    "X = df[TEXT_COL].astype(str).values\n",
    "y = df[LABEL_COL].astype(int).values\n",
    "\n",
    "# stratified split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=TEST_SIZE, random_state=RANDOM_STATE, stratify=y\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0aee68c4-c3b1-4e37-81c4-3fe909d424b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF shape: (7531, 25900)\n"
     ]
    }
   ],
   "source": [
    "# ---------- TF-IDF vectorization ----------\n",
    "vectorizer = TfidfVectorizer(ngram_range=TFIDF_NGRAMS, max_features=MAX_FEATURES, min_df=2)\n",
    "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
    "X_test_tfidf = vectorizer.transform(X_test)\n",
    "\n",
    "print(\"TF-IDF shape:\", X_train_tfidf.shape)\n",
    "\n",
    "# Optional: SMOTE (uncomment if you want to use oversampling)\n",
    "if USE_SMOTE:\n",
    "    from imblearn.over_sampling import SMOTE\n",
    "    sm = SMOTE(random_state=RANDOM_STATE)\n",
    "    X_train_tfidf, y_train = sm.fit_resample(X_train_tfidf, y_train)\n",
    "    print(\"After SMOTE, train shape:\", X_train_tfidf.shape, np.bincount(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "873d43e2-5224-419d-b92d-8099f06eced7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- Utility: evaluation printer ----------\n",
    "def evaluate_and_print(name, model, X_val, y_val):\n",
    "    \"\"\"\n",
    "    Print classification report + PR AUC and confusion matrix.\n",
    "    If model has predict_proba use that for PR-AUC; otherwise use decision_function.\n",
    "    \"\"\"\n",
    "    y_pred = model.predict(X_val)\n",
    "    print(f\"\\n=== {name} ===\")\n",
    "    print(classification_report(y_val, y_pred, digits=4))\n",
    "    # Confusion matrix\n",
    "    cm = confusion_matrix(y_val, y_pred)\n",
    "    print(\"Confusion matrix:\\n\", cm)\n",
    "    # PR AUC\n",
    "    try:\n",
    "        if hasattr(model, \"predict_proba\"):\n",
    "            y_scores = model.predict_proba(X_val)[:, 1]\n",
    "        else:\n",
    "            # fallback to decision_function (SVM)\n",
    "            y_scores = model.decision_function(X_val)\n",
    "        precision, recall, _ = precision_recall_curve(y_val, y_scores)\n",
    "        pr_auc = auc(recall, precision)\n",
    "        print(f\"Precision-Recall AUC: {pr_auc:.4f}\")\n",
    "    except Exception as e:\n",
    "        print(\"Could not compute PR-AUC:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "861b1f81-e63b-49cf-bec9-69dcd7be79cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix, precision_recall_curve, auc\n",
    "import os\n",
    "\n",
    "# ---------- Utility: evaluation printer + file writer ----------\n",
    "def evaluate_and_print(name, model, X_val, y_val, output_file=\"model_results_voting_ensemble.txt\"):\n",
    "    \"\"\"\n",
    "    Print classification report + PR AUC and confusion matrix.\n",
    "    Store results in an external file (append mode).\n",
    "    \"\"\"\n",
    "    y_pred = model.predict(X_val)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "\n",
    "    print(f\"{name} Results:\")\n",
    "    print(\"Accuracy:\", accuracy)\n",
    "    print(\"Precision:\", precision)\n",
    "    print(\"Recall:\", recall)\n",
    "    print(\"-\" * 30)\n",
    "\n",
    "    # Append results to global list\n",
    "    results.append({\n",
    "        \"model\": name,\n",
    "        \"accuracy\": accuracy,\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall\n",
    "    })\n",
    "    # Open file in append mode\n",
    "    with open(output_file, \"a\") as f:\n",
    "        f.write(f\"\\n=== {name} ===\\n\")\n",
    "        report = classification_report(y_val, y_pred, digits=4)\n",
    "        f.write(report + \"\\n\")\n",
    "\n",
    "        # Confusion matrix\n",
    "        cm = confusion_matrix(y_val, y_pred)\n",
    "        f.write(\"Confusion matrix:\\n\" + str(cm) + \"\\n\")\n",
    "\n",
    "        # PR AUC\n",
    "        try:\n",
    "            if hasattr(model, \"predict_proba\"):\n",
    "                y_scores = model.predict_proba(X_val)[:, 1]\n",
    "            else:\n",
    "                y_scores = model.decision_function(X_val)\n",
    "            precision, recall, _ = precision_recall_curve(y_val, y_scores)\n",
    "            pr_auc = auc(recall, precision)\n",
    "            f.write(f\"Precision-Recall AUC: {pr_auc:.4f}\\n\")\n",
    "        except Exception as e:\n",
    "            f.write(f\"Could not compute PR-AUC: {e}\\n\")\n",
    "\n",
    "    # Still print to console for convenience\n",
    "    print(f\"\\n=== {name} ===\")\n",
    "    print(report)\n",
    "    print(\"Confusion matrix:\\n\", cm)\n",
    "    try:\n",
    "        print(f\"Precision-Recall AUC: {pr_auc:.4f}\")\n",
    "    except:\n",
    "        print(\"Could not compute PR-AUC\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6e52250f-e66c-4764-ab14-1d7d9e528892",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training XGBoost...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/xgboost/training.py:183: UserWarning: [14:55:15] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost Results:\n",
      "Accuracy: 0.9341476367498672\n",
      "Precision: 0.6830357142857143\n",
      "Recall: 0.7427184466019418\n",
      "------------------------------\n",
      "\n",
      "=== XGBoost ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9681    0.9577    0.9628      1677\n",
      "           1     0.6830    0.7427    0.7116       206\n",
      "\n",
      "    accuracy                         0.9341      1883\n",
      "   macro avg     0.8255    0.8502    0.8372      1883\n",
      "weighted avg     0.9369    0.9341    0.9353      1883\n",
      "\n",
      "Confusion matrix:\n",
      " [[1606   71]\n",
      " [  53  153]]\n",
      "Precision-Recall AUC: 0.7615\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['./models/xgb_claudette.joblib']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ---------- Model 1: XGBoost ----------\n",
    "# handle imbalance via scale_pos_weight = (neg_count / pos_count)\n",
    "neg = (y_train == 0).sum()\n",
    "pos = (y_train == 1).sum()\n",
    "scale_pos_weight = neg / max(1, pos)\n",
    "\n",
    "xgb = XGBClassifier(\n",
    "    n_estimators=300,\n",
    "    max_depth=6,\n",
    "    learning_rate=0.05,\n",
    "    objective=\"binary:logistic\",\n",
    "    eval_metric=\"logloss\",\n",
    "    use_label_encoder=False,\n",
    "    scale_pos_weight=scale_pos_weight,\n",
    "    random_state=RANDOM_STATE,\n",
    "    n_jobs=-1,\n",
    ")\n",
    "print(\"Training XGBoost...\")\n",
    "xgb.fit(X_train_tfidf, y_train)\n",
    "evaluate_and_print(\"XGBoost\", xgb, X_test_tfidf, y_test)\n",
    "joblib.dump(xgb, os.path.join(SAVE_DIR, \"xgb_claudette.joblib\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "66ecdbe8-38c4-41fd-83b8-cba60eebf49d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training LightGBM...\n",
      "[LightGBM] [Info] Number of positive: 826, number of negative: 6705\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.073413 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 66807\n",
      "[LightGBM] [Info] Number of data points in the train set: 7531, number of used features: 2420\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[50]\tvalid_0's binary_logloss: 0.299598\n",
      "[100]\tvalid_0's binary_logloss: 0.217378\n",
      "[150]\tvalid_0's binary_logloss: 0.186309\n",
      "[200]\tvalid_0's binary_logloss: 0.171015\n",
      "[250]\tvalid_0's binary_logloss: 0.162707\n",
      "[300]\tvalid_0's binary_logloss: 0.159531\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[291]\tvalid_0's binary_logloss: 0.159334\n",
      "LightGBM Results:\n",
      "Accuracy: 0.944768985661179\n",
      "Precision: 0.755\n",
      "Recall: 0.7330097087378641\n",
      "------------------------------\n",
      "\n",
      "=== LightGBM ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9673    0.9708    0.9690      1677\n",
      "           1     0.7550    0.7330    0.7438       206\n",
      "\n",
      "    accuracy                         0.9448      1883\n",
      "   macro avg     0.8612    0.8519    0.8564      1883\n",
      "weighted avg     0.9441    0.9448    0.9444      1883\n",
      "\n",
      "Confusion matrix:\n",
      " [[1628   49]\n",
      " [  55  151]]\n",
      "Precision-Recall AUC: 0.8032\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['./models/lgbm_claudette.joblib']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ---------- Model 2: LightGBM ----------\n",
    "from lightgbm import early_stopping, log_evaluation\n",
    "lgbm = LGBMClassifier(\n",
    "    n_estimators=300,\n",
    "    max_depth=-1,\n",
    "    learning_rate=0.05,\n",
    "    objective=\"binary\",\n",
    "    class_weight=\"balanced\",  # alternatively use scale_pos_weight\n",
    "    random_state=RANDOM_STATE,\n",
    "    n_jobs=-1,\n",
    ")\n",
    "print(\"Training LightGBM...\")\n",
    "lgbm.fit(\n",
    "    X_train_tfidf, \n",
    "    y_train,\n",
    "    eval_set=[(X_test_tfidf, y_test)],\n",
    "    callbacks=[early_stopping(stopping_rounds=30), log_evaluation(50)]\n",
    ")\n",
    "evaluate_and_print(\"LightGBM\", lgbm, X_test_tfidf, y_test)\n",
    "joblib.dump(lgbm, os.path.join(SAVE_DIR, \"lgbm_claudette.joblib\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d09a3d2b-198d-4e51-8a9d-ef925391005e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training SVM (LinearSVC) with calibration (this can take some time)...\n",
      "SVM (Calibrated LinearSVC) Results:\n",
      "Accuracy: 0.9548592671269251\n",
      "Precision: 0.8622754491017964\n",
      "Recall: 0.6990291262135923\n",
      "------------------------------\n",
      "\n",
      "=== SVM (Calibrated LinearSVC) ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9639    0.9863    0.9749      1677\n",
      "           1     0.8623    0.6990    0.7721       206\n",
      "\n",
      "    accuracy                         0.9549      1883\n",
      "   macro avg     0.9131    0.8427    0.8735      1883\n",
      "weighted avg     0.9528    0.9549    0.9528      1883\n",
      "\n",
      "Confusion matrix:\n",
      " [[1654   23]\n",
      " [  62  144]]\n",
      "Precision-Recall AUC: 0.8297\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['./models/svm_calibrated_claudette.joblib']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# ---------- Model 3: SVM (LinearSVC) with calibration ----------\n",
    "# LinearSVC is fast and supports sparse input; class_weight='balanced' to handle class imbalance\n",
    "svc = LinearSVC(class_weight=\"balanced\", max_iter=20000, random_state=RANDOM_STATE)\n",
    "# Calibrate to get probabilities (useful for PR curves / voting soft)\n",
    "svc_cal = CalibratedClassifierCV(estimator=svc, cv=3, method=\"sigmoid\")\n",
    "print(\"Training SVM (LinearSVC) with calibration (this can take some time)...\")\n",
    "svc_cal.fit(X_train_tfidf, y_train)\n",
    "evaluate_and_print(\"SVM (Calibrated LinearSVC)\", svc_cal, X_test_tfidf, y_test)\n",
    "joblib.dump(svc_cal, os.path.join(SAVE_DIR, \"svm_calibrated_claudette.joblib\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d3c04105-6f5c-4e6b-94d7-60de16affd82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Voting ensemble (soft voting)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/xgboost/training.py:183: UserWarning: [14:55:43] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Voting Ensemble (XGB+LGBM+SVM) Results:\n",
      "Accuracy: 0.9468932554434413\n",
      "Precision: 0.7676767676767676\n",
      "Recall: 0.7378640776699029\n",
      "------------------------------\n",
      "\n",
      "=== Voting Ensemble (XGB+LGBM+SVM) ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9680    0.9726    0.9703      1677\n",
      "           1     0.7677    0.7379    0.7525       206\n",
      "\n",
      "    accuracy                         0.9469      1883\n",
      "   macro avg     0.8678    0.8552    0.8614      1883\n",
      "weighted avg     0.9460    0.9469    0.9464      1883\n",
      "\n",
      "Confusion matrix:\n",
      " [[1631   46]\n",
      " [  54  152]]\n",
      "Precision-Recall AUC: 0.8183\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['./models/voting_ensemble_claudette.joblib']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ---------- Ensemble: Soft Voting (XGB + LGBM + SVM-calibrated) ----------\n",
    "# Create fresh estimator instances (voting will fit them), but it's okay to reuse trained ones if you prefer to skip refit\n",
    "voting = VotingClassifier(\n",
    "    estimators=[\n",
    "        (\"xgb\", XGBClassifier(\n",
    "            n_estimators=200, max_depth=6, learning_rate=0.05,\n",
    "            use_label_encoder=False, eval_metric=\"logloss\", scale_pos_weight=scale_pos_weight, random_state=RANDOM_STATE)),\n",
    "        (\"lgbm\", LGBMClassifier(n_estimators=200, learning_rate=0.05, class_weight=\"balanced\", random_state=RANDOM_STATE)),\n",
    "        (\"svm\", CalibratedClassifierCV(estimator=LinearSVC(class_weight=\"balanced\", max_iter=20000, random_state=RANDOM_STATE), cv=3)),\n",
    "    ],\n",
    "    voting=\"soft\",\n",
    "    n_jobs=-1,\n",
    ")\n",
    "print(\"Training Voting ensemble (soft voting)...\")\n",
    "voting.fit(X_train_tfidf, y_train)\n",
    "evaluate_and_print(\"Voting Ensemble (XGB+LGBM+SVM)\", voting, X_test_tfidf, y_test)\n",
    "joblib.dump(voting, os.path.join(SAVE_DIR, \"voting_ensemble_claudette.joblib\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f117ddbc-45c0-4ffc-bee0-60ad15456efd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All models + vectorizer saved to ./models\n",
      "XGBoost best threshold (on test) for F1 ~ 0.7226 at threshold 0.6313\n"
     ]
    }
   ],
   "source": [
    "# ---------- Save vectorizer ----------\n",
    "joblib.dump(vectorizer, os.path.join(SAVE_DIR, \"tfidf_vectorizer.joblib\"))\n",
    "print(\"All models + vectorizer saved to\", SAVE_DIR)\n",
    "\n",
    "# ---------- Optional: adjust decision threshold for best F1 (example for xgb) ----------\n",
    "def find_best_threshold(model, X_val, y_val):\n",
    "    # returns threshold that maximizes F1\n",
    "    if hasattr(model, \"predict_proba\"):\n",
    "        probs = model.predict_proba(X_val)[:, 1]\n",
    "    else:\n",
    "        probs = model.decision_function(X_val)\n",
    "    precision, recall, thresholds = precision_recall_curve(y_val, probs)\n",
    "    f1_scores = 2 * (precision * recall) / (precision + recall + 1e-12)\n",
    "    best_idx = np.nanargmax(f1_scores)\n",
    "    best_threshold = thresholds[best_idx] if best_idx < len(thresholds) else 0.5\n",
    "    return best_threshold, f1_scores[best_idx]\n",
    "\n",
    "best_thr, best_f1 = find_best_threshold(xgb, X_test_tfidf, y_test)\n",
    "print(f\"XGBoost best threshold (on test) for F1 ~ {best_f1:.4f} at threshold {best_thr:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ecfc5b7b-4b98-46c8-8b72-59ba21d2f297",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'model': 'XGBoost',\n",
       "  'accuracy': 0.9341476367498672,\n",
       "  'precision': 0.6830357142857143,\n",
       "  'recall': 0.7427184466019418},\n",
       " {'model': 'LightGBM',\n",
       "  'accuracy': 0.944768985661179,\n",
       "  'precision': 0.755,\n",
       "  'recall': 0.7330097087378641},\n",
       " {'model': 'SVM (Calibrated LinearSVC)',\n",
       "  'accuracy': 0.9548592671269251,\n",
       "  'precision': 0.8622754491017964,\n",
       "  'recall': 0.6990291262135923},\n",
       " {'model': 'Voting Ensemble (XGB+LGBM+SVM)',\n",
       "  'accuracy': 0.9468932554434413,\n",
       "  'precision': 0.7676767676767676,\n",
       "  'recall': 0.7378640776699029}]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f0bd866e-d809-4d02-83f1-ac22a91d06b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            Model  Accuracy  Precision    Recall\n",
      "0                         XGBoost  0.934148   0.683036  0.742718\n",
      "1                        LightGBM  0.944769   0.755000  0.733010\n",
      "2      SVM (Calibrated LinearSVC)  0.954859   0.862275  0.699029\n",
      "3  Voting Ensemble (XGB+LGBM+SVM)  0.946893   0.767677  0.737864\n"
     ]
    }
   ],
   "source": [
    "#storing the results back into the file\n",
    "# storing the results back into the file (append mode)\n",
    "df_results = pd.DataFrame(results)\n",
    "\n",
    "# Rename columns to match your CSV headers\n",
    "df_results.rename(columns={\n",
    "    \"model\": \"Model\",\n",
    "    \"accuracy\": \"Accuracy\",\n",
    "    \"precision\": \"Precision\",\n",
    "    \"recall\": \"Recall\"\n",
    "}, inplace=True)\n",
    "\n",
    "\n",
    "file_path = \"model_results_voting_ensemble_table.csv\"\n",
    "\n",
    "if not os.path.isfile(file_path):\n",
    "    # File does not exist -> write with header\n",
    "    df_results.to_csv(file_path, index=False, mode=\"w\")\n",
    "else:\n",
    "    # File exists -> append without header\n",
    "    df_results.to_csv(file_path, index=False, mode=\"a\", header=False)\n",
    "\n",
    "print(df_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d878268-e445-4cdc-a28a-078842d93e55",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "460bf112-2fd2-49a5-bbe2-197874519832",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53d8d31a-1833-4392-87f1-dc37712749cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3befe39-1ca1-4c2b-be2b-293aa0e55a04",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
